\documentclass[11pt, a4paper]{article}
\usepackage{amsmath, amsthm, amssymb, geometry}
\geometry{left=14mm, right=14mm, top=22mm, bottom=22mm}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{proposition}{Proposition}[section]
\begin{document}

\title{Appendix on bfcm (Filtering Model)}
\author{jiyucho9145}
\maketitle
\begin{abstract}
We define a model (Filtering Model) for calculating a conditional probability of a catgory given a message,
and show that the model coinsides with Bayesian Model on specific conditions.
\end{abstract}

\newpage
\tableofcontents

\newpage
\section{Introduction}

Some mail software classify mails into several folders automatically.
It's classification algorithm is often implemented by a classification model which is a kind of Bayesian Model,
and satisfies 2 specific assumptions.

In this paper, we have defined a classification model (Filtering Model) without the assumptions,
and have shown that the new classification model generalizes the ordinal classification model.
But, The new classification model is slow, and is not practical for mail classification.

\section{Filtering Model}

\subsection{Filtering Model}

\begin{definition}
Let $ W $ be an nonempty finite set, $ E = \mathrm{Map}(W, \{0, 1\}), C = \{c_{1}, c_{2}), c_{1} \ne c_{2} $,
then we call $ (W, E, C) $ a filtering model.
\end{definition}

\begin{definition}
Let $ g : F \to C, F \subset E, F \ne \O $, then we call $ g : F \to C $ a training data for a filtering model $ (W, E, C) $.
\end{definition}

\begin{definition}
Let $ e \in F $, then we define a support $ \mathrm{Supp}_{g}(e) $ of e as follows:
\begin{equation}
\mathrm{Supp}_{g}(e) = \{ w \in W ; e(w) \ne 0\}.
\end{equation}
\end{definition}

\begin{definition}
Let $ Z \subset W $, then we define a set $ M_{g}(Z) $ as follows:
\begin{equation}
M_{g}(Z) = \{e \in F ; Z \subset \mathrm{Supp}_{g}(e)\}.
\end{equation}
\end{definition}

\begin{definition}
Let $ w \in W $, then we define a set $ M_{g}(w) $ as follows:
\begin{equation}
M_{g}(w) = M_{g}(\{w\}).
\end{equation}
\end{definition}

\subsection{Filtering Probability}

\subsection{Filtering Conditional Probability}

\subsection{Calculationg Conditional Probability on Specific Conditions}

%\begin{thebibliography}{99}
%\bibitem{J}{Joel Grus, Data Science from Scratch: First Princeples with Python, O'Reilly (2019)}
%\end{thebibliography}

\end{document}
