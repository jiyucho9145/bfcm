\documentclass[11pt, a4note]{article}
\usepackage{amsmath, amsthm, amssymb, geometry}
\geometry{left=14mm, right=14mm, top=22mm, bottom=22mm}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{theorem}{Theorem}[section]
\begin{document}

\title{A Note on an Elementary Formulation of the Naive Bayesian Model}
\author{jiyucho9145}
\maketitle
\begin{abstract}
Some mail server programs and some mail client programs classify messages into several categories
by the naive Bayesian classifier automatically. The algorithm of the naive Bayesian classifier is based on
the naive Bayesian model, which satisfies the two specific assumptions.

In this note, we define a probabilistic model (message receiving model) without the two assumptions
and show that the message receiving model coincides with the naive Bayesian model under the two assumptions.
\end{abstract}

\newpage
\tableofcontents

\newpage
\section{Introduction}
The naive Bayesian classifier is used in some mail server programs and some mail client programs.
Many Japanese books regarding to machine leaning have been published in recent years.
Several popular machine larning algorithms are introduced in these books.
The naive Bayesian classifier is one of these algorithms.

The mathematical background of the naive Bayesian classifier is not introduced in some books.
This policy is appropriate in practice because programmers are able to implement and use
the naive Bayesian classifier without the mathematical background.

However, we have an interest in the mathematical background of the naive Bayesian classifier. 
We formulate the naive Bayesian model in elementary way by probability theory
because the naive Bayesian classifier is based on the naive Bayesian model.

Next section introduces a thory on a probabilistic model (message receiving model),
which is a base of the theory on the naive Bayesian model, section 3 formulates
the naive Bayesian model.

\section{Message Receiving Model}

\subsection{Message Receiving Model}

\begin{definition}
Let $ W $ be an nonempty finite set, $ E = \mathrm{Map}(W, \{0, 1\}), C = \{c_{1}, c_{2} \}, c_{1} \ne c_{2} $,
then we call $ (W, E, C) $ a message receiving model in this note.
\end{definition}

\begin{definition}
Let $ g : F \to C, F \subset E, F \ne \O, F_{g,i} = g^{-1}(c_{i}) \ne \O $, then we call $ g : F \to C $
a training data for a message receiving model $ (W, E, C) $ in this note.
\end{definition}

\begin{definition}
Let $ e \in E $, then we define a support $ \mathrm{Supp}(e) $ of $ e $ as follows:
\begin{equation}
\mathrm{Supp}(e) = \{ w \in W ; e(w) \ne 0\}.
\end{equation}
\end{definition}

\begin{definition}
Let $ Z \subset W $, then we define a set $ M_{g}(Z) $ as follows:
\begin{equation}
M_{g}(Z) = \{e \in F ; Z \subset \mathrm{Supp}(e)\}.
\end{equation}
\end{definition}

\begin{definition}
Let $ w \in W $, then we define a set $ M_{g}(w) $ as follows:
\begin{equation}
M_{g}(w) = M_{g}(\{w\}).
\end{equation}
\end{definition}

\begin{definition}
Let $ e \in E $, then we define a set $ M_{g}(e) $ as follows:
\begin{equation}
M_{g}(e) = \cap_{w \in \mathrm{Supp}(e)}M_{g}(w).
\end{equation}
\end{definition}

\subsection{Message Receiving Measures}

\begin{definition}
We call the map
\begin{equation}
m_{g} : \mathrm{Pow}(F) \to \mathbb{R} ; G \mapsto \mathrm{card}(G)
\end{equation}
a message receiving measure of $ g : F \to C $ in this note.
Where $ \mathrm{Pow}(S) $ denots the power set of $ S $ for arbitrary set $ S $.
\end{definition}

\begin{proposition}
$ m_{g} : \mathrm{Pow}(F) \to \mathbb{R} $ is a measure on measurable space $ (F, \mathrm{Pow}(F)) $.
\end{proposition}

\begin{proposition}
\begin{equation}
\mathrm{card}(F_{g,1}) + \mathrm{card}(F_{g,2}) = \mathrm{card}(F).
\end{equation}
\end{proposition}

\subsection{Message Receiving Probabilities}

\begin{definition}
We call the map
\begin{equation}
P_{g} : \mathrm{Pow}(F) \to \mathbb{R} ; G \mapsto m(G)/m(F)
\end{equation}
a message receiving probability of $ g : F \to C $ in this note.
\end{definition}

\begin{proposition}
$ P_{g} : \mathrm{Pow}(F) \to \mathbb{R} $ is a probability (measure) on measurable space $ (F, \mathrm{Pow}(F)) $.
\end{proposition}

\begin{proposition}
\begin{equation}
P_{g}(F_{g,1}) + P_{g}(F_{g,2}) = 1.
\end{equation}
\end{proposition}

\subsection{Message Receiving Conditional Probabilities}

\begin{definition}
We call the conditional probability 
\begin{equation}
P_{g}(-|-): \mathrm{Pow}(F) \times \mathrm{Pow}(F) \to \mathbb{R}
\end{equation}
a message receiving conditional probability of $ g : F \to C $ in this note.
\end{definition}

\begin{proposition}
For arbitrary $ M, N \subset F $, following equation is satisfied:
\begin{equation}
P_{g}(M \cap N) = P_{g}(M | N)P_{g}(N).
\end{equation}
\end{proposition}

\begin{proposition}
For arbitrary $ M \subset F $, following equation is satisfied:
\begin{equation}
P_{g}(F_{g,1}|M) = \frac{P_{g}(M|F_{g,1})P(F_{g,1})}{P_{g}(M|F_{g,1})P(F_{g,1}) + P_{g}(M|F_{g,2})P(F_{g,2})}.
\end{equation}
\end{proposition}

\begin{proposition}
For arbitrary $ M \subset F $, following equation is satisfied:
\begin{equation}
P_{g}(F_{g,1}|M) + P_{g}(F_{g,2}|M) = 1.
\end{equation}
\end{proposition}

\section{Comparation with Naive Bayesian Model}

\subsection{Calculationg Conditional Probabilities under the Two Specific Assumptions}

\begin{theorem}
Assume that $ g : F \to C $ satisfies following two conditions:
(i) $ P_{g}(F_{g,1}) = P_{g}(F_{g,2}) = 1/2 $;
(ii) for arbitrary $ w_{1}, w_{2}, \dots, w_{r} \in W $,
\begin{equation}
P_{g}(M_{g}(\{ w_{1}, w_{2}, \dots, w_{r} \})|F_{g,i})
= P_{g}(M_{g}(w_{1})|F_{g,i})P_{g}(M_{g}(w_{2})|F_{g,i}) \cdots P_{g}(M_{g}(w_{r})|F_{g,i}).
\end{equation}
i.e, $ M_{g}(w_{1}), M_{g}(w_{2}), \dots, M_{g}(w_{r}) $ are independent.
And, let $ Q_{g}(w_{1}, w_{2}, \dots, w_{r}), R_{g}(w_{1}, w_{2}, \dots, w_{r}) $ be following functions:
\begin{eqnarray}
Q_{g}(w_{1}, w_{2}, \dots, w_{r}) = \prod_{i}^{r}P_{g}(M(w_{i})|F_{g,1}), \\
R_{g}(w_{1}, w_{2}, \dots, w_{r}) = \prod_{i}^{r}(\frac{P_{g}(M(w_{i})) - P_{g}(M(w_{i})|F_{g,1})P_{g}(F_{g,1})}{1 - P_{g}(F_{g,1})}).
\end{eqnarray}
Then, for arbitrary $ e \in E $, the following equation is satisfied
\begin{equation}
P_{g}(F_{g,1}|M_{g}(e))
= \frac{Q_{g}(w_{1}, w_{2}, \dots, w_{r})}{Q_{g}(w_{1}, w_{2}, \dots, w_{r}) + R_{g}(w_{1}, w_{2}, \dots, w_{r})}.
\end{equation}
Where $ \mathrm{Supp}(e) = \{ w_{1}, w_{2}, \dots, w_{r} \} \subset W $.
\end{theorem}

\end{document}
