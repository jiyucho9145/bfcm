\documentclass[11pt, a4note]{article}
\usepackage{amsmath, amsthm, amssymb, geometry}
\geometry{left=14mm, right=14mm, top=22mm, bottom=22mm}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{proposition}{Proposition}[section]
\begin{document}

\title{An Elementary Formulation of the Naive Bayesian Model}
\author{jiyucho9145}
\maketitle
\begin{abstract}
Some mail server programs and some mail client programs classify messages into several categories
by the naive Bayesian classifier automatically. The algorithm of the naive Bayesian classifier is based on
the naive Bayesian model, which satisfies the two specific assumptions.

In this note, we define a probabilistic model (message receiving model) without the two assumptions
and show that the message receiving model coincides with the naive Bayesian model under the two assumptions.
\end{abstract}

\newpage
\tableofcontents

\newpage
\section{Introduction}

\section{Message Receiving Model}

\subsection{Message Receiving Model}

\begin{definition}
Let $ W $ be an nonempty finite set, $ E = \mathrm{Map}(W, \{0, 1\}), C = \{c_{1}, c_{2}), c_{1} \ne c_{2} $,
then we call $ (W, E, C) $ a message receiving model.
\end{definition}

\begin{definition}
Let $ g : F \to C, F \subset E, F \ne \O $, then we call $ g : F \to C $ a training data for a message receiving model $ (W, E, C) $.
\end{definition}

\begin{definition}
Let $ e \in F $, then we define a support $ \mathrm{Supp}_{g}(e) $ of e as follows:
\begin{equation}
\mathrm{Supp}_{g}(e) = \{ w \in W ; e(w) \ne 0\}.
\end{equation}
\end{definition}

\begin{definition}
Let $ Z \subset W $, then we define a set $ M_{g}(Z) $ as follows:
\begin{equation}
M_{g}(Z) = \{e \in F ; Z \subset \mathrm{Supp}_{g}(e)\}.
\end{equation}
\end{definition}

\begin{definition}
Let $ w \in W $, then we define a set $ M_{g}(w) $ as follows:
\begin{equation}
M_{g}(w) = M_{g}(\{w\}).
\end{equation}
\end{definition}

\subsection{Message Receiving Measures}

\subsection{Message Receiving Probabilities}

\subsection{Message Receiving Conditional Probabilities}

\section{Comparation with Naive Bayesian Model}

\subsection{Calculationg Conditional Probabilities under the Two Specific Assumptions}

%\begin{thebibliography}{99}
%\bibitem{J}{Joel Grus, Data Science from Scratch: First Princeples with Python, O'Reilly (2019)}
%\end{thebibliography}

\end{document}
