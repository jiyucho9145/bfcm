\documentclass[11pt, a4note]{article}
\usepackage{amsmath, amsthm, amssymb, geometry}
\geometry{left=14mm, right=14mm, top=22mm, bottom=22mm}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{proposition}{Proposition}[section]
\begin{document}

\title{An Elementary Formulation of the Naive Bayesian Model}
\author{jiyucho9145}
\maketitle
\begin{abstract}
Some mail server programs and some mail client programs classify messages into several categories
by the naive Bayesian classifier automatically. The algorithm of the naive Bayesian classifier is based on
the naive Bayesian model, which satisfies the two specific assumptions.

In this note, we define a probabilistic model (message receiving model) without the two assumptions
and show that the message receiving model coincides with the naive Bayesian model under the two assumptions.
\end{abstract}

\newpage
\tableofcontents

\newpage
\section{Introduction}

\section{Message Receiving Model}

\subsection{Message Receiving Model}

\begin{definition}
Let $ W $ be an nonempty finite set, $ E = \mathrm{Map}(W, \{0, 1\}), C = \{c_{1}, c_{2}), c_{1} \ne c_{2} $,
then we call $ (W, E, C) $ a message receiving model.
\end{definition}

\begin{definition}
Let $ g : F \to C, F \subset E, F \ne \O $, then we call $ g : F \to C $ a training data for a message receiving model $ (W, E, C) $.
\end{definition}

\begin{definition}
Let $ e \in F $, then we define a support $ \mathrm{Supp}_{g}(e) $ of e as follows:
\begin{equation}
\mathrm{Supp}_{g}(e) = \{ w \in W ; e(w) \ne 0\}.
\end{equation}
\end{definition}

\begin{definition}
Let $ Z \subset W $, then we define a set $ M_{g}(Z) $ as follows:
\begin{equation}
M_{g}(Z) = \{e \in F ; Z \subset \mathrm{Supp}_{g}(e)\}.
\end{equation}
\end{definition}

\begin{definition}
Let $ w \in W $, then we define a set $ M_{g}(w) $ as follows:
\begin{equation}
M_{g}(w) = M_{g}(\{w\}).
\end{equation}
\end{definition}

\subsection{Message Receiving Measures}

\begin{definition}
We call the map
\begin{equation}
m_{g} : \mathrm{Pow}(F) \to \mathbb{R} ; G \mapsto \mathrm{card}(G)
\end{equation}
a message receiving measure of $ g : F \to C $.
Where $ \mathrm{Pow}(S) $ denots the power set of $ S $ for arbitrary set $ S $.
\end{definition}

\begin{proposition}
$ m_{g} : \mathrm{Pow}(F) \to \mathbb{R} $ is a measure on measurable space $ (F, \mathrm{Pow}(F)) $.
\end{proposition}

\begin{proposition}
Let $ C_{g,i} = g^{-1}(c_{i}) $, then
\begin{equation}
\mathrm{card}(C_{g,1}) + \mathrm{card}(C_{g,2}) = \mathrm{card}(F).
\end{equation}
\end{proposition}

\subsection{Message Receiving Probabilities}

\begin{definition}
We call the map
\begin{equation}
P_{g} : \mathrm{Pow}(F) \to \mathbb{R} ; G \mapsto m(G)/m(F)
\end{equation}
a message receiving probability of $ g : F \to C $.
\end{definition}

\begin{proposition}
$ P_{g} : \mathrm{Pow}(F) \to \mathbb{R} $ is a probability (measure) on measurable space $ (F, \mathrm{Pow}(F)) $.
\end{proposition}

\begin{proposition}
\begin{equation}
P_{g}(C_{g,1}) + P_{g}(C_{g,2}) = 1.
\end{equation}
\end{proposition}

\subsection{Message Receiving Conditional Probabilities}

\begin{definition}
We call the conditional probability 
\begin{equation}
P_{g}(-|-): \mathrm{Pow}(F) \times \mathrm{Pow}(F) \to \mathbb{R}
\end{equation}
a message receiving conditional probability of $ g : F \to C $.
\end{definition}

\begin{proposition}
For arbitrary $ w \in W $, following equation is satisfied:
\begin{equation}
P_{g}(C_{g,1}|M(w)) + P_{g}(C_{g,2}|M(w)) = 1.
\end{equation}
\end{proposition}

\section{Comparation with Naive Bayesian Model}

\subsection{Calculationg Conditional Probabilities under the Two Specific Assumptions}

%\begin{thebibliography}{99}
%\bibitem{J}{Joel Grus, Data Science from Scratch: First Princeples with Python, O'Reilly (2019)}
%\end{thebibliography}

\end{document}
